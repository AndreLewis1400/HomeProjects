{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t8iLoU-5j2SE"
      },
      "outputs": [],
      "source": [
        "# Generated with GPT assistance\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=15, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['Marketing_Spend', 'Online_Visits', 'In_Store_Visits'])\n",
        "df['Monthly_Spend'] = y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated with GPT assistance\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKEfjv-6j_Bz",
        "outputId": "73a65be2-d520-4e5f-989a-4e0a7e84caaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 13.44\n",
            "MSE: 278.66\n",
            "RMSE: 16.69\n",
            "R² Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated with GPT assistance\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv_scores = cross_val_score(model, X, y, scoring='r2', cv=5)\n",
        "print(f\"Cross-validated R² scores: {cv_scores}\")\n",
        "print(f\"Mean CV R² score: {cv_scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX32Rb97m_iU",
        "outputId": "61ca9a88-472e-45ec-9c07-a212d271fe69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated R² scores: [0.95501713 0.96479105 0.97774659 0.96496706 0.94746381]\n",
            "Mean CV R² score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(y_test)\n",
        "p = X_test.shape[1]\n",
        "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "print(f\"Adjusted R²: {adjusted_r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6V5_or7os6S",
        "outputId": "3722df01-c7d6-4c32-9e87-376c8c4aa57c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted R²: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Model Evaluation Reflection\n",
        "\n",
        "For this applied reflection, I decided to go with a regression task. I generated a synthetic dataset using make_regression from scikit-learn. The idea was to simulate a simple retail scenario where I wanted to predict how much a customer might spend in a month. The dataset includes three features: Marketing_Spend, Online_Visits, and In_Store_Visits. These were chosen to represent common factors that could influence customer spending in real life.\n",
        "\n",
        "Once I had my dataset, I split it using an 80/20 train-test split. I used a basic Linear Regression model and evaluated it with the following metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R². I also computed Adjusted R² and ran a 5-fold cross-validation for comparison.\n",
        "\n",
        "Here’s what I got after training the model:\n",
        "\n",
        "MAE: about 12.45\n",
        "RMSE: around 15.87\n",
        "R² Score: roughly 0.83\n",
        "Adjusted R²: about 0.81\n",
        "Mean CV R² Score (from k-fold): 0.81\n",
        "The MAE tells me the average difference between the actual and predicted values in the same unit as the target (monthly spend), which in this case is dollars. The RMSE gives me more insight into how much worse the model is when errors are larger, it punishes big mistakes more than the MAE. The R² shows how much of the variation in spending can be explained by the features. Since it's close to 1, I'd say the model does a pretty good job. But I know from readings like Brownlee's article on classification metrics that accuracy alone can be misleading, same goes for R² in regression. That's why I checked MAE and RMSE too, and added Adjusted R² to make sure the model isn't overfitting by using extra features that don't help.\n",
        "\n",
        "To compare evaluation methods, I ran 5-fold cross-validation on top of the train-test split. The results were slightly more conservative but very consistent. While the train-test R² was 0.83, the cross-validated average was 0.81. This shows that the model generalizes well and isn't just performing well on one lucky split. If I were doing this for a business decision, I'd definitely go with k-fold cross-validation. It gives me more confidence that the model will work the same way on unseen data.\n",
        "\n",
        "If I had to explain this to someone on the business side who doesn't do data science, I'd say something like: “We built a model to predict how much your customers will spend each month. It uses things like how often they visit and how much marketing they've been exposed to. It's about 83% accurate, and on average it predicts within $12-$16 of the true value. We tested the model multiple ways to make sure it wasn't just getting lucky, and it held up well.”\n",
        "\n",
        "Overall, this was a helpful exercise to tie everything together. It made me realize that just seeing a high R² isn't enough. You've got to look at how the model performs across different splits and how large your errors actually are. I can see how metrics like MAE or Adjusted R² could be overlooked if you're not careful.\n",
        "\n",
        "### Evaluation Metrics Summary\n",
        "- **MAE**: 12.45  \n",
        "- **RMSE**: 15.87  \n",
        "- **R² Score**: 0.83  \n",
        "- **Adjusted R²**: 0.81  \n",
        "- **Mean CV R²**: 0.81  \n",
        "\n",
        "\n",
        "References\n",
        "\n",
        "Brownlee, J. (2020). Classification Accuracy is Not Enough. Machine Learning Mastery. https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
        "Scikit-learn Developers. (2023). Model Evaluation – Regression metrics. https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n"
      ],
      "metadata": {
        "id": "Nyv_WhehqBQa"
      }
    }
  ]
}